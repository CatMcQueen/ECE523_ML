# -*- coding: utf-8 -*-
"""RealBeeClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SIH718KpJVflrGtfYQ8hy6-t5Q2slFuc

##Bee Identification
"""

import numpy as np
import os
import matplotlib.pyplot as plt

from sklearn.metrics import classification_report, confusion_matrix
from PIL import Image

from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
from tensorflow.keras.layers import Dense, Dropout, Flatten, GaussianNoise, AveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from keras.callbacks import EarlyStopping, ModelCheckpoint

from google.colab import drive
drive.mount('/content/gdrive')

TRAIN_PATH = '/content/gdrive/MyDrive/small/train'
TEST_PATH  = '/content/gdrive/MyDrive/small/test'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
IMAGE_CHANNEL = 3
EPOCHS = 15

files = os.listdir(TRAIN_PATH)
OUTPUT_CLASSES = len(files)
print(files)

Conv_Base = ResNet50( weights = 'imagenet')

for layer in Conv_Base.layers[:-8]:
    layer.trainable = False

model = Sequential()
model.add(GaussianNoise(.5))
model.add(Conv_Base)
#model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(units = 256, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(units = OUTPUT_CLASSES, activation = 'softmax'))

"""##Train the Species Label"""

train_datagen = ImageDataGenerator(
    shear_range=10,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    preprocessing_function=preprocess_input)

train_generator = train_datagen.flow_from_directory(
    TRAIN_PATH,
    batch_size=32,
    class_mode='binary',
    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH))

validation_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input)

validation_generator = validation_datagen.flow_from_directory(
    TEST_PATH,
    shuffle=False,
    class_mode='binary',
    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH))

optimizer = Adam()
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

from keras.callbacks import EarlyStopping, ModelCheckpoint
callbacks = [
             EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10),
              ModelCheckpoint('/content/gdrive/MyDrive/small/best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=False)
             ]

history = model.fit(train_generator,
                    epochs=EPOCHS,
                    validation_data=validation_generator, callbacks=callbacks)

model.summary()

train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']
No_Of_Epochs = range(EPOCHS)

plt.figure()
plt.plot(No_Of_Epochs, train_acc, marker = '.', color = 'blue', markersize = 12, 
                 linewidth = 2, label = 'Training Accuracy')
plt.plot(No_Of_Epochs, val_acc, marker = '.', color = 'red', markersize = 12, 
                 linewidth = 2, label = 'Validation Accuracy')

plt.title('Accuracy w.r.t Number of Epochs')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()

plt.figure()

plt.plot(No_Of_Epochs, train_loss, marker = '.', color = 'blue', markersize = 12, 
                 linewidth = 2, label = 'Training Loss')
plt.plot(No_Of_Epochs, val_loss, marker = '.', color = 'red', markersize = 12, 
                 linewidth = 2, label = 'Validation Loss')


plt.title('Loss w.r.t Number of Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

validation_img_paths = []
for tgtclass in files:
  filename = TEST_PATH + '/' + tgtclass
  images = os.listdir(filename)
  index  = np.random.randint(0, len(images)-1) 
  imagepath = TEST_PATH + '/' + tgtclass + '/' + images[index] 
  validation_img_paths.append(imagepath )

img_list = [Image.open(img_path) for img_path in validation_img_paths]
validation_batch = np.stack([preprocess_input(np.array(img.resize((IMAGE_HEIGHT,IMAGE_WIDTH))))
                             for img in img_list])

pred_probs = model.predict(validation_batch)

plt.figure( figsize=(10, 5))
half = (int)(len(img_list)/2)
#fig, axs = plt.subplots(2, half, figsize=(20, 5))
j = 0
for i, img in enumerate(img_list):
    if (i > half):
      j = 1
    ax =  plt.subplot(2, half, i + 1)
    ax.axis('off')
    maxitem = np.argmax(pred_probs[i])
    ax.set_title("{:.0f} {}".format(pred_probs[i,maxitem], files[maxitem][:-2]))
    ax.imshow(img)

plt.figure( figsize=(10, 5))
half = (int)(len(img_list)/2)
#fig, axs = plt.subplots(2, half, figsize=(20, 5))
j = 0
for i, img in enumerate(img_list):
    if (i > half):
      j = 1
    ax =  plt.subplot(2, half, i + 1)
    ax.axis('off')
    maxitem = np.argmax(pred_probs[i])
    ax.set_title("{:.0f}% {}".format(100*pred_probs[i,maxitem], files[maxitem][:-2]))
    ax.imshow(img)

num_test_samples = 500
batch_size = 32
Y_pred = model.predict(validation_generator, num_test_samples // batch_size+1)
y_pred = np.argmax(Y_pred, axis=1)
print('Confusion Matrix')
print(confusion_matrix(validation_generator.classes, y_pred))
print('Classification Report')
target_names = files
print(classification_report(validation_generator.classes, y_pred, target_names=target_names))

"""## Doing The Genus

Map the Genuses together
"""

names = train_generator.class_indices
print(train_generator.class_indices)

genuslist = []
for fi in files:
  name = fi.split('_')
  genuslist.append(name[0])

genuslist = np.unique(genuslist)
genuslist = np.ndarray.tolist(genuslist)
genuslist.sort()

keys_from_dict = list(names.values())
names_from_dict  = list(names.keys())

mapping_comb = []
for i, fi in enumerate(names_from_dict):
  nm     = fi.split('_')
  val    = genuslist.index(nm[0])
  mapping_comb.append(val)

old_to_new = mapping_comb

OUTPUT_CLASSES_G = len(genuslist)

print(old_to_new)

Conv_Baseg = ResNet50( weights = 'imagenet')

for layer in Conv_Baseg.layers[:-8]:
    layer.trainable = False

modelg = Sequential()
modelg.add(GaussianNoise(.5))
modelg.add(Conv_Baseg)
modelg.add(Flatten())
modelg.add(Dense(units = 256, activation = 'relu'))
modelg.add(Dropout(0.5))
modelg.add(Dense(units = OUTPUT_CLASSES_G, activation = 'softmax'))

optimizer = Adam()
modelg.compile(loss='sparse_categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

from keras.callbacks import EarlyStopping, ModelCheckpoint
callbacksg = [
             EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10),
             ModelCheckpoint('/content/gdrive/MyDrive/small/best_model_g.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=False)
             ]

# the wrapping generator
def new_gen(gen):
  for data, labels in gen:
    print(data) 
    print(labels)
    labels = [old_to_new[int(n)] for n in labels]
    labels = np.array(labels)
    # now you can call np_utils.to_categorical method 
    # if you would like one-hot encoded labels
    yield data, labels

import math
EPOCHSG = 10
BATCH_SIZE = 32
compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / BATCH_SIZE))

# use the number of samples to determine the number of steps per epoch
# this is done for you when you're just doing the train_generator
TRAINING_SIZE = train_generator.samples 
VALIDATION_SIZE = validation_generator.samples

steps_per_epoch = compute_steps_per_epoch(TRAINING_SIZE)
val_steps = compute_steps_per_epoch(VALIDATION_SIZE)

historyg = modelg.fit(new_gen(train_generator),
                    epochs=EPOCHSG,
                    steps_per_epoch=steps_per_epoch,
                    validation_data=new_gen(validation_generator), 
                    validation_steps=val_steps, callbacks=callbacksg)

modelg.summary()

train_acc_g = historyg.history['accuracy']
val_acc_g = historyg.history['val_accuracy']
train_loss_g = historyg.history['loss']
val_loss_g = historyg.history['val_loss']
No_Of_Epochs = range(EPOCHSG)

plt.figure()
plt.plot(No_Of_Epochs, train_acc_g, marker = '.', color = 'blue', markersize = 12, 
                 linewidth = 2, label = 'Training Accuracy')
plt.plot(No_Of_Epochs, val_acc_g, marker = '.', color = 'red', markersize = 12, 
                 linewidth = 2, label = 'Validation Accuracy')

plt.title('Genus Accuracy w.r.t Number of Epochs')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()

plt.figure()

plt.plot(No_Of_Epochs, train_loss_g, marker = '.', color = 'blue', markersize = 12, 
                 linewidth = 2, label = 'Training Loss')
plt.plot(No_Of_Epochs, val_loss_g, marker = '.', color = 'red', markersize = 12, 
                 linewidth = 2, label = 'Validation Loss')


plt.title('Genus Loss w.r.t Number of Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

validation_img_paths = []
for tgtclass in files:
  filename = TEST_PATH + '/' + tgtclass
  images = os.listdir(filename)
  index  = np.random.randint(0, len(images)-1) 
  imagepath = TEST_PATH + '/' + tgtclass + '/' + images[index] 
  validation_img_paths.append(imagepath )

img_list = [Image.open(img_path) for img_path in validation_img_paths]
validation_batch = np.stack([preprocess_input(np.array(img.resize((IMAGE_HEIGHT,IMAGE_WIDTH))))
                             for img in img_list])

pred_probs = modelg.predict(validation_batch)

plt.figure( figsize=(6, 5))
half = (int)(len(img_list)/2)
#fig, axs = plt.subplots(2, half, figsize=(20, 5))
j = 0
for i, img in enumerate(img_list):
    if (i > half):
      j = 1
    ax =  plt.subplot(2, half, i + 1)
    ax.axis('off')
    maxitem = np.argmax(pred_probs[i])
    ax.set_title("{:.0f}% {}".format(100*pred_probs[i,maxitem], genuslist[maxitem]))
    ax.imshow(img)

num_test_samples = 500
batch_size = 32
Y_pred = model.predict(validation_generator, num_test_samples // batch_size+1)
y_pred = np.argmax(Y_pred, axis=1)

print('Confusion Matrix')
outclass = np.ndarray.tolist(validation_generator.classes)
outputs = [old_to_new[y] for y in outclass]
print(confusion_matrix(outputs, y_pred))

print('Classification Report')
print(classification_report(outputs, y_pred))

